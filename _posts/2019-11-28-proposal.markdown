---
layout: post
title:  "Proposal"
date:   2019-11-28 00:57:49 +0000
categories: proposal omr
---

## Background
Composers, performers and musicians frequently want to make edits, comments, and changes to the sheet music. Doing so traditionally will produce hundreds or thousands of uncluttered pages of sheet music. Digital format is the perfect solution to better organize a collection of sheet music. It ensures replayability, enables large-scale musical theory analysis, and allows users to search for specific content systematically. Therefore, we present an end-to-end system to digitize music collection. 

## Current Status
# Pipeline
1. Preprocessing: contrast enhancement, binarization, noise removal, skew correction, etc.
2. Object Detection: finding and classifying all relevant symbols in the image
3. Musical Semantic Reconstruction: reconstruct logical relationship (e.g. between a notehead and a stem) and temporal relationship
4. Encoding: encode to a MIDI or MusicXML format

# Previous Studies
1. Staff Line Removal: connected component analysis, [auto-encoders][auto-encoders]
2. Bounding Box Prediction: [region-based CNN][handwritten] (Faster R-CNN), [semantic segmentation model][unet] (U-Net), [watershed semantic segmentation][watershed] (Deep Watershed)

[
<img src="https://www.mdpi.com/applsci/applsci-08-01488/article_deploy/html/images/applsci-08-01488-g005.png" alt="DeepScores" width='230' height='300'>
|
<img src="https://www.mdpi.com/applsci/applsci-08-01488/article_deploy/html/images/applsci-08-01488-g006.png" alt="Muscima" width='230' height='300'> 
|
<img src="https://www.mdpi.com/applsci/applsci-08-01488/article_deploy/html/images/applsci-08-01488-g007.png" alt="Capitan" width='230' height='300'>
]

Results in terms of mAP (%) and w-mAP (%) with respect to the dataset and object detector model following the COCO evaluation protocol.

<table>
<thead><tr><th align='center' valign='middle' style='border-bottom:solid thin;border-top:solid thin' class='html-align-center'> </th><th colspan='3' align='center' valign='middle' style='border-bottom:solid thin;border-top:solid thin' class='html-align-center'>mAP (%)</th><th colspan='3' align='center' valign='middle' style='border-bottom:solid thin;border-top:solid thin' class='html-align-center'>w-mAP (%)</th></tr><tr><th align='center' valign='middle' style='border-bottom:solid thin' class='html-align-center'> </th><th align='center' valign='middle' style='border-bottom:solid thin' class='html-align-center'>DeepScores</th><th align='center' valign='middle' style='border-bottom:solid thin' class='html-align-center'>MUSCIMA++</th><th align='center' valign='middle' style='border-bottom:solid thin' class='html-align-center'>Capitan</th><th align='center' valign='middle' style='border-bottom:solid thin' class='html-align-center'>DeepScores</th><th align='center' valign='middle' style='border-bottom:solid thin' class='html-align-center'>MUSCIMA++</th><th align='center' valign='middle' style='border-bottom:solid thin' class='html-align-center'>Capitan</th></tr></thead><tbody><tr><td align='center' valign='middle' class='html-align-center'><b>Faster R-CNN</b></td><td align='center' valign='middle' class='html-align-center'>19.6</td><td align='center' valign='middle' class='html-align-center'>3.9</td><td align='center' valign='middle' class='html-align-center'>15.2</td><td align='center' valign='middle' class='html-align-center'>14.4</td><td align='center' valign='middle' class='html-align-center'>7.9</td><td align='center' valign='middle' class='html-align-center'>23.2</td></tr><tr><td align='center' valign='middle' class='html-align-center'><b>RetinaNet</b></td><td align='center' valign='middle' class='html-align-center'>9.8</td><td align='center' valign='middle' class='html-align-center'>7.7</td><td align='center' valign='middle' class='html-align-center'>14.5</td><td align='center' valign='middle' class='html-align-center'>1.9</td><td align='center' valign='middle' class='html-align-center'>4.9</td><td align='center' valign='middle' class='html-align-center'>34.9</td></tr><tr><td align='center' valign='middle' style='border-bottom:solid thin' class='html-align-center'><b>U-Net</b></td><td align='center' valign='middle' style='border-bottom:solid thin' class='html-align-center'>24.8</td><td align='center' valign='middle' style='border-bottom:solid thin' class='html-align-center'>16.6</td><td align='center' valign='middle' style='border-bottom:solid thin' class='html-align-center'>17.4</td><td align='center' valign='middle' style='border-bottom:solid thin' class='html-align-center'>23.3</td><td align='center' valign='middle' style='border-bottom:solid thin' class='html-align-center'>33.6</td><td align='center' valign='middle' style='border-bottom:solid thin' class='html-align-center'>26.0</td></tr></tbody>
</table>

# Challenges
1. Unbalanced dataset: rare symbols
2. Notation variability
3. Musical symbol isolation: multiple symbols could be connected to each other (e.g. a beam group) or a single unit can have multiple disconnected parts (e.g. vermata, voltas)
4. High density of more than 1,000 objects in a single page

## Methods
# Dataset
1. [DeepScores][deepscores]: huge synthetic dataset in Common Western Modern Notation (CWMN), consisting of 300,000 images for performing symbol classification, image segmentation, and object detection
2. [MUSCIMA++][muscima]: handwritten sheet music dataset in CWMN, consisting of over 90,000 images for symbol detection and symbol semantic relationships

## Reference

1. [A Baseline for General Music Object Detection with Deep Learning][omr-baseline]
2. [Understanding Optical Music Recognition][omr-exp]
3. [Gated-SCNN: Gated Shape CNNs for Semantic Segmentation][gated-scnn]

[omr-exp]: https://arxiv.org/pdf/1908.03608.pdf
[deepscores]: https://tuggeluk.github.io/deepscores/
[omr-baseline]: https://www.mdpi.com/2076-3417/8/9/1488
[muscima]: https://ufal.mff.cuni.cz/muscima
[auto-encoders]: https://github.com/ajgallego/staff-lines-removal
[handwritten]: https://ieeexplore.ieee.org/abstract/document/8395189
[unet]: http://ismir2018.ismir.net/doc/pdfs/175_Paper.pdf
[watershed]: https://arxiv.org/abs/1805.10548
[gated-scnn]: https://arxiv.org/pdf/1907.05740.pdf
