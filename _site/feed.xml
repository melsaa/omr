<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="http://localhost:4000/omr/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/omr/" rel="alternate" type="text/html" /><updated>2019-12-28T16:11:52+00:00</updated><id>http://localhost:4000/omr/feed.xml</id><title type="html">Optical Music Recognition</title><subtitle>We transform sheet music into a digital format.</subtitle><entry><title type="html">Notation Assembly</title><link href="http://localhost:4000/omr/2019/12/28/notation-assembly.html" rel="alternate" type="text/html" title="Notation Assembly" /><published>2019-12-28T14:18:09+00:00</published><updated>2019-12-28T14:18:09+00:00</updated><id>http://localhost:4000/omr/2019/12/28/notation-assembly</id><content type="html" xml:base="http://localhost:4000/omr/2019/12/28/notation-assembly.html">&lt;h2 id=&quot;definition&quot;&gt;Definition&lt;/h2&gt;
&lt;p&gt;Notation Assembly is the process of recovering the music notation semantics from the detected and classified symbols. The output is a symbolic representation of the symbols and their relationships, typically as a graph.&lt;/p&gt;

&lt;h2 id=&quot;current-approaches&quot;&gt;Current Approaches&lt;/h2&gt;
&lt;p&gt;Due to the complexity of how musical semantics are inferred from the image, it is difficult (for now) to formulate it as a learnable optimization problem. While end-to-end systems for OMR do exist, they are still limited to a subset of music notation, at best. Some recent works have considered deep recurrent neural networks for monophonic music written in both typeset and handwritten modern notation.&lt;/p&gt;

&lt;h3 id=&quot;using-a-grammar-for-a-reliable-full-score-recognition-system-1995&quot;&gt;Using A Grammar for A Reliable Full Score Recognition System (1995)&lt;/h3&gt;
&lt;p&gt;Propose a grammar to formalize the musical knowledge on full scores with &lt;strong&gt;polyphonic&lt;/strong&gt; staves.
The objects on a score can be divided into 2 categories:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Constructs: composed of &lt;strong&gt;segments&lt;/strong&gt; (stems, beams) and notehead&lt;/li&gt;
  &lt;li&gt;Symbolic: clefs, accidentals, etc. These can be considered as characters and recognized by using OCR&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;graphical-part&quot;&gt;Graphical Part&lt;/h1&gt;
&lt;p&gt;Example: rules for &lt;code class=&quot;highlighter-rouge&quot;&gt;beamedNote&lt;/code&gt; &lt;br /&gt;
&lt;img src=&quot;https://i.imgur.com/OMviEnn.jpg&quot; alt=&quot;beamedNote&quot; width=&quot;450&quot; height=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;syntactic-part&quot;&gt;Syntactic Part&lt;/h1&gt;
&lt;p&gt;Introduce the notion of Step, which corresponds to the smallest duration in a column of vertically aligned notes. It can manage the simultaneity (i.e. the verticality) of notes on the same staff (in case of polyphony), and on the full scores. Informations associated to the staff (i.e. dynamics, slurs) are introduced in the grammar at Step level. &lt;br /&gt;
&lt;img src=&quot;https://imgur.com/OUm7j1a.jpg&quot; alt=&quot;syntactic&quot; width=&quot;400&quot; height=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;staves-system&quot;&gt;Staves System&lt;/h1&gt;
&lt;p&gt;Assign each musical symbol to the right bar
&lt;img src=&quot;https://i.imgur.com/vbvG61W.jpg&quot; alt=&quot;staffMap&quot; width=&quot;400&quot; height=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;a-starting-point-for-handwritten-music-recognition-2018&quot;&gt;A Starting Point for Handwritten Music Recognition (2018)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Input: handwritten music scores annotated at symbol level (need to re-label the dataset)&lt;/li&gt;
  &lt;li&gt;Output: binary matrix corresponds to rhythm (80 classes) and pitch(28 classes)&lt;/li&gt;
  &lt;li&gt;Rhythm: 4, 3, 2, 1, 0.5, …, dynamics, slurs, rests, sharp&lt;/li&gt;
  &lt;li&gt;Pitch: C1, D1, …&lt;/li&gt;
  &lt;li&gt;Pipeline: Input -&amp;gt; Conv_block -&amp;gt; Bi-LSTM -&amp;gt; Dense_Layers -&amp;gt; Output&lt;/li&gt;
  &lt;li&gt;Staves are cropped and processed independently, resized to 100px height&lt;/li&gt;
  &lt;li&gt;Use Convolutional Neural Network to extract meaningful features for each staves&lt;/li&gt;
  &lt;li&gt;Use Bidirectional LSTM to recover the musical semantics&lt;/li&gt;
  &lt;li&gt;Chords are translated into sequences: [Start Chord], [quarterNote, Line1], [quarterNote, Space2], [quarterNote, Line4], [End Chord]&lt;/li&gt;
  &lt;li&gt;Transfer learning from typeset to handwritten musical notation&lt;/li&gt;
  &lt;li&gt;Evaluation metric: Symbol Error Rate (SER) similar to Word Error Rate (WER)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;towards-full-pipeline-handwritten-omr-with-musical-symbol-detection-by-u-nets-2018&quot;&gt;Towards Full-Pipeline Handwritten OMR with Musical Symbol Detection by U-Nets (2018)&lt;/h3&gt;
&lt;p&gt;Pipeline: input -&amp;gt; symbol detection (U-Net) -&amp;gt; recover graph edges -&amp;gt; recover precedence edges -&amp;gt; convert to MIDI file&lt;/p&gt;

&lt;h1 id=&quot;symbol-detection&quot;&gt;Symbol Detection&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Propose Convex-Hull segmentation targets, works good to detect f-clefs and c-clefs &lt;br /&gt;
&lt;img src=&quot;https://imgur.com/mDNMDR7.jpg&quot; alt=&quot;convex&quot; width=&quot;400&quot; height=&quot;200&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Using U-Net is viable, but is limited by the size of receptive field: the middle of a long stem looks exactly the same as a barline&lt;/li&gt;
  &lt;li&gt;Further research: leverage syntactic properties of music notation: self-attention layer that allows building up the final output from partial recognition results&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;notation-assembly-and-pitch-inference&quot;&gt;Notation Assembly and Pitch Inference&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;MUSIMA++ semantics are represented as an oriented graph, once the graph is recovered, one can perform deterministic pitch inference&lt;/li&gt;
  &lt;li&gt;Symbol detection outputs vertices of the notation graph; the next step therefore is to recover graph edges&lt;/li&gt;
  &lt;li&gt;Method: same as [here][muscima], train a binary classifier over ordered symbol pairs&lt;/li&gt;
  &lt;li&gt;Drawback: make embarrassing erros, i.e. connect noteheads with irrelevant symbols (multiple adjacent stems, beams from different staff)&lt;/li&gt;
  &lt;li&gt;Further research: combine with grammar rules to reduce errors&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1908.03608.pdf&quot;&gt;Understanding Optical Music Recognition&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=F44590BADB2A8997A735F3E5F4391416?doi=10.1.1.29.4927&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Using A Grammar for A Reliable Full Score Recognition System&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/pdf?id=SygqKLQrXQ&quot;&gt;A Starting Point for Handwritten Music Recognition&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pdfs.semanticscholar.org/f05c/b3674df33d35e562ca79b9b3af2e10c1a88a.pdf&quot;&gt;Towards Full-Pipeline Handwritten OMR with Musical Symbol Detection by U-Nets&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1507.05717.pdf&quot;&gt;An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.mdpi.com/2076-3417/8/4/606&quot;&gt;End-to-End Neural Optical Music Recognition of Monophonic Scores&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><summary type="html">Definition Notation Assembly is the process of recovering the music notation semantics from the detected and classified symbols. The output is a symbolic representation of the symbols and their relationships, typically as a graph.</summary></entry><entry><title type="html">Proposal</title><link href="http://localhost:4000/omr/proposal/omr/2019/11/28/proposal.html" rel="alternate" type="text/html" title="Proposal" /><published>2019-11-28T00:57:49+00:00</published><updated>2019-11-28T00:57:49+00:00</updated><id>http://localhost:4000/omr/proposal/omr/2019/11/28/proposal</id><content type="html" xml:base="http://localhost:4000/omr/proposal/omr/2019/11/28/proposal.html">&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;
&lt;p&gt;Composers, performers and musicians frequently want to make edits, comments, and changes to the sheet music. Doing so traditionally will produce hundreds or thousands of uncluttered pages of sheet music. Digital format is the perfect solution to better organize a collection of sheet music. It ensures replayability, enables large-scale musical theory analysis, and allows users to search for specific content systematically. Therefore, we present an end-to-end system to digitize music collection.&lt;/p&gt;

&lt;h2 id=&quot;current-status&quot;&gt;Current Status&lt;/h2&gt;
&lt;h1 id=&quot;pipeline&quot;&gt;Pipeline&lt;/h1&gt;
&lt;ol&gt;
  &lt;li&gt;Preprocessing: contrast enhancement, binarization, noise removal, skew correction, etc.&lt;/li&gt;
  &lt;li&gt;Object Detection: finding and classifying all relevant symbols in the image&lt;/li&gt;
  &lt;li&gt;Musical Semantic Reconstruction: reconstruct logical relationship (e.g. between a notehead and a stem) and temporal relationship&lt;/li&gt;
  &lt;li&gt;Encoding: encode to a MIDI or MusicXML format&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;previous-studies&quot;&gt;Previous Studies&lt;/h1&gt;
&lt;ol&gt;
  &lt;li&gt;Staff Line Removal: connected component analysis, &lt;a href=&quot;https://github.com/ajgallego/staff-lines-removal&quot;&gt;auto-encoders&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Bounding Box Prediction: &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/8395189&quot;&gt;region-based CNN&lt;/a&gt; (Faster R-CNN), &lt;a href=&quot;http://ismir2018.ismir.net/doc/pdfs/175_Paper.pdf&quot;&gt;semantic segmentation model&lt;/a&gt; (U-Net), &lt;a href=&quot;https://arxiv.org/abs/1805.10548&quot;&gt;watershed semantic segmentation&lt;/a&gt; (Deep Watershed)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;[
&lt;img src=&quot;https://www.mdpi.com/applsci/applsci-08-01488/article_deploy/html/images/applsci-08-01488-g005.png&quot; alt=&quot;DeepScores&quot; width=&quot;230&quot; height=&quot;300&quot; /&gt;
|
&lt;img src=&quot;https://www.mdpi.com/applsci/applsci-08-01488/article_deploy/html/images/applsci-08-01488-g006.png&quot; alt=&quot;Muscima&quot; width=&quot;230&quot; height=&quot;300&quot; /&gt; 
|
&lt;img src=&quot;https://www.mdpi.com/applsci/applsci-08-01488/article_deploy/html/images/applsci-08-01488-g007.png&quot; alt=&quot;Capitan&quot; width=&quot;230&quot; height=&quot;300&quot; /&gt;
]&lt;/p&gt;

&lt;p&gt;Results in terms of mAP (%) and w-mAP (%) with respect to the dataset and object detector model following the COCO evaluation protocol.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; class=&quot;html-align-center&quot;&gt; &lt;/th&gt;&lt;th colspan=&quot;3&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; class=&quot;html-align-center&quot;&gt;mAP (%)&lt;/th&gt;&lt;th colspan=&quot;3&quot; align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin;border-top:solid thin&quot; class=&quot;html-align-center&quot;&gt;w-mAP (%)&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; class=&quot;html-align-center&quot;&gt; &lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; class=&quot;html-align-center&quot;&gt;DeepScores&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; class=&quot;html-align-center&quot;&gt;MUSCIMA++&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; class=&quot;html-align-center&quot;&gt;Capitan&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; class=&quot;html-align-center&quot;&gt;DeepScores&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; class=&quot;html-align-center&quot;&gt;MUSCIMA++&lt;/th&gt;&lt;th align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; class=&quot;html-align-center&quot;&gt;Capitan&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; class=&quot;html-align-center&quot;&gt;&lt;b&gt;Faster R-CNN&lt;/b&gt;&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; class=&quot;html-align-center&quot;&gt;19.6&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; class=&quot;html-align-center&quot;&gt;3.9&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; class=&quot;html-align-center&quot;&gt;15.2&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; class=&quot;html-align-center&quot;&gt;14.4&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; class=&quot;html-align-center&quot;&gt;7.9&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; class=&quot;html-align-center&quot;&gt;23.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; class=&quot;html-align-center&quot;&gt;&lt;b&gt;RetinaNet&lt;/b&gt;&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; class=&quot;html-align-center&quot;&gt;9.8&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; class=&quot;html-align-center&quot;&gt;7.7&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; class=&quot;html-align-center&quot;&gt;14.5&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; class=&quot;html-align-center&quot;&gt;1.9&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; class=&quot;html-align-center&quot;&gt;4.9&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; class=&quot;html-align-center&quot;&gt;34.9&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; class=&quot;html-align-center&quot;&gt;&lt;b&gt;U-Net&lt;/b&gt;&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; class=&quot;html-align-center&quot;&gt;24.8&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; class=&quot;html-align-center&quot;&gt;16.6&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; class=&quot;html-align-center&quot;&gt;17.4&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; class=&quot;html-align-center&quot;&gt;23.3&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; class=&quot;html-align-center&quot;&gt;33.6&lt;/td&gt;&lt;td align=&quot;center&quot; valign=&quot;middle&quot; style=&quot;border-bottom:solid thin&quot; class=&quot;html-align-center&quot;&gt;26.0&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;challenges&quot;&gt;Challenges&lt;/h1&gt;
&lt;ol&gt;
  &lt;li&gt;Unbalanced dataset: rare symbols&lt;/li&gt;
  &lt;li&gt;Notation variability&lt;/li&gt;
  &lt;li&gt;Musical symbol isolation: multiple symbols could be connected to each other (e.g. a beam group) or a single unit can have multiple disconnected parts (e.g. vermata, voltas)&lt;/li&gt;
  &lt;li&gt;High density of more than 1,000 objects in a single page&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt;
&lt;h1 id=&quot;dataset&quot;&gt;Dataset&lt;/h1&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://tuggeluk.github.io/deepscores/&quot;&gt;DeepScores&lt;/a&gt;: huge synthetic dataset in Common Western Modern Notation (CWMN), consisting of 300,000 images for performing symbol classification, image segmentation, and object detection&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ufal.mff.cuni.cz/muscima&quot;&gt;MUSCIMA++&lt;/a&gt;: handwritten sheet music dataset in CWMN, consisting of over 90,000 images for symbol detection and symbol semantic relationships&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.mdpi.com/2076-3417/8/9/1488&quot;&gt;A Baseline for General Music Object Detection with Deep Learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1908.03608.pdf&quot;&gt;Understanding Optical Music Recognition&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1907.05740.pdf&quot;&gt;Gated-SCNN: Gated Shape CNNs for Semantic Segmentation&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><summary type="html">Background Composers, performers and musicians frequently want to make edits, comments, and changes to the sheet music. Doing so traditionally will produce hundreds or thousands of uncluttered pages of sheet music. Digital format is the perfect solution to better organize a collection of sheet music. It ensures replayability, enables large-scale musical theory analysis, and allows users to search for specific content systematically. Therefore, we present an end-to-end system to digitize music collection.</summary></entry></feed>